{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DpLDSRFjEUE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random as rand\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, TFBertForSequenceClassification, AutoModelForMaskedLM\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0YnybPriUKg"
   },
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def func_betolt(lr, Trainable, train_dataset, val_dataset, test_dataset, tokenizer):\n",
    "        # Load BERT tokenizer\n",
    "        # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Load BERT model\n",
    "        model = TFBertForSequenceClassification.from_pretrained(\"yorko/scibert_scivocab_uncased_long_4096\", from_pt=True)\n",
    "        \n",
    "        # Set up optimizer and loss function\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        \n",
    "        model.layers[0].trainable = Trainable\n",
    "        print (\"Learning rate: \" + str(lr) + \"    Trainable: \" + str(Trainable))\n",
    "        model.summary()\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['sparse_categorical_accuracy'])\n",
    "        es = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True)\n",
    "        hist = model.fit(train_dataset, epochs=1000, \n",
    "                validation_data=val_dataset,\n",
    "                callbacks=[es],\n",
    "                verbose=1)\n",
    "        \n",
    "        #plt.plot(hist.history[\"loss\"])\n",
    "        #plt.plot(hist.history[\"val_loss\"])\n",
    "        vissza = [len(hist.history[\"loss\"]), model.evaluate(train_dataset, verbose=0), model.evaluate(val_dataset, verbose=0), model.evaluate(test_dataset, verbose=0)]\n",
    "        return vissza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'title_abstract_keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/train_{}.pkl\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Comparing measurement properties of EQ-5D-Y-3L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Feasibility of the EQ-5D in the elderly popula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Comparing the self-reported health-related qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Testing measurement properties of two EQ-5D yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Use of Antimalarial Agents is Associated with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Comparing measurement properties of EQ-5D-Y-3L...\n",
       "1      0  Feasibility of the EQ-5D in the elderly popula...\n",
       "2      1  Comparing the self-reported health-related qua...\n",
       "3      1  Testing measurement properties of two EQ-5D yo...\n",
       "4      1  Use of Antimalarial Agents is Associated with ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Comparing measurement properties of EQ-5D-Y-3L and EQ-5D-Y-5L in paediatric patients [SEP] BACKGROUND: The adult versions EQ-5D-3L and EQ-5D-5L have been extensive compared. This is not the case for the EQ-5D youth versions. The study aim was to compare the measurement properties and responsiveness of EQ-5D-Y-3L and EQ-5D-Y-5L in paediatric patients. METHODS: A sample of patients 8-16\\xa0years old with different diseases and a wide range of disease severity was asked to complete EQ-5D-Y-3L, EQ-5D-Y-5L, PedsQL Generic Core Scale, and selected, appropriate disease-specific instruments, three times. EQ-5D-Y-3L and EQ-5D-Y-5L were compared in terms of: feasibility, (re-)distribution properties, discriminatory power, convergent validity, test-retest reliability, and responsiveness. RESULTS: 286 participating patients suffered from one of the following diseases: major beta-thalassemia, haemophilia, acute lymphoblastic leukaemia, acute illness. Missing responses were comparable between versions of the EQ-5D-Y, suggesting comparable feasibility. The number of patients in the best health state (level profile 11111) was equal in both EQ-5D-Y versions. The projection of EQ-5D-Y-3L scores onto EQ-5D-Y-5L for all dimensions showed that the two additional levels in EQ-5D-Y-5L slightly improved the accuracy of patients in reporting their problems, especially if severe. Convergent validity with PedsQL and disease-specific measures showed that the two EQ-5D-Y versions performed about equally. Test-retest reliability (EQ-5D-Y-3L 0.78 vs EQ-5D-Y-5L 0.84), and sensitivity for detecting health changes, were both better in EQ-5D-Y-5L. CONCLUSIONS: Extending the number of levels did not give clear superiority to EQ-5D-Y-5L over EQ-5D-Y-3L based on the criteria assessed in this study. However, increasing the number of levels benefitted EQ-5D-Y performance in the measurement of moderate to severe problems and especially in longitudinal study designs [SEP] Adolescent; Adult; Child; Humans; Longitudinal Studies; Psychometrics; Quality of Life; Reproducibility of Results; Surveys and Questionnaires; EQ-5D-Y-3L paediatric patients; EQ-5D-Y-5L; Health-related quality of life; Psychometrics'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#random stratified validation subset split\n",
    "#_diff = 1\n",
    "#while _diff >= .02:\n",
    "#    tts = train_dataset.train_test_split(test_size=.15, shuffle=True)\n",
    "#    _train_ratio, _val_ratio = np.sum(tts[\"train\"][\"label\"]) / len(tts[\"train\"][\"label\"]), np.sum(tts[\"test\"][\"label\"]) / len(tts[\"test\"][\"label\"])\n",
    "#    _diff = abs(_train_ratio - _val_ratio)\n",
    "#    print(_train_ratio, _val_ratio, _diff)\n",
    "#\n",
    "#train_dataset = tts[\"train\"]\n",
    "#val_dataset = tts[\"test\"]\n",
    "\n",
    "\n",
    "#subsets should be fixed for all tests\n",
    "_val_ids = [2, 7, 24, 32, 36, 47, 49, 59, 61, 71, 72, 86, 90, 95, 96]\n",
    "train_dataset = Dataset.from_pandas(df[~df.index.isin(_val_ids)])\n",
    "val_dataset = Dataset.from_pandas(df[df.index.isin(_val_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.611764705882353, 0.6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_dataset[\"label\"]) / len(train_dataset[\"label\"]), np.sum(val_dataset[\"label\"]) / len(val_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': 'Comparing measurement properties of EQ-5D-Y-3L and EQ-5D-Y-5L in paediatric patients [SEP] BACKGROUND: The adult versions EQ-5D-3L and EQ-5D-5L have been extensive compared. This is not the case for the EQ-5D youth versions. The study aim was to compare the measurement properties and responsiveness of EQ-5D-Y-3L and EQ-5D-Y-5L in paediatric patients. METHODS: A sample of patients 8-16\\xa0years old with different diseases and a wide range of disease severity was asked to complete EQ-5D-Y-3L, EQ-5D-Y-5L, PedsQL Generic Core Scale, and selected, appropriate disease-specific instruments, three times. EQ-5D-Y-3L and EQ-5D-Y-5L were compared in terms of: feasibility, (re-)distribution properties, discriminatory power, convergent validity, test-retest reliability, and responsiveness. RESULTS: 286 participating patients suffered from one of the following diseases: major beta-thalassemia, haemophilia, acute lymphoblastic leukaemia, acute illness. Missing responses were comparable between versions of the EQ-5D-Y, suggesting comparable feasibility. The number of patients in the best health state (level profile 11111) was equal in both EQ-5D-Y versions. The projection of EQ-5D-Y-3L scores onto EQ-5D-Y-5L for all dimensions showed that the two additional levels in EQ-5D-Y-5L slightly improved the accuracy of patients in reporting their problems, especially if severe. Convergent validity with PedsQL and disease-specific measures showed that the two EQ-5D-Y versions performed about equally. Test-retest reliability (EQ-5D-Y-3L 0.78 vs EQ-5D-Y-5L 0.84), and sensitivity for detecting health changes, were both better in EQ-5D-Y-5L. CONCLUSIONS: Extending the number of levels did not give clear superiority to EQ-5D-Y-5L over EQ-5D-Y-3L based on the criteria assessed in this study. However, increasing the number of levels benefitted EQ-5D-Y performance in the measurement of moderate to severe problems and especially in longitudinal study designs [SEP] Adolescent; Adult; Child; Humans; Longitudinal Studies; Psychometrics; Quality of Life; Reproducibility of Results; Surveys and Questionnaires; EQ-5D-Y-3L paediatric patients; EQ-5D-Y-5L; Health-related quality of life; Psychometrics',\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/test_{}.pkl\".format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_dataset[\"label\"]) / len(test_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh1ICeIiykZv"
   },
   "source": [
    "# Preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116,
     "referenced_widgets": [
      "4136d11d23304f44a6fd77bd5078c678",
      "858f4eafdb214acd80236294e009f285",
      "a38b3162339e4d49a7c705818d3d9014",
      "b7a8f10c56f84fce8fd9e0325b3c444e",
      "0e5baf42d613466c9789d6ed74223515",
      "cbfb95452bce4570805ef71e5fce0121",
      "a3cddd07fc284418982f9bd6cba0e89f",
      "d1fd9a54bdb847a3a4ac6a9e49f8bea6",
      "49684e0c97d34a558c268306fbbdf3f4",
      "ac8ce475408a49669d263c85042f9530",
      "b033809b6f434f12b338b3ac2119da37",
      "09d3283c59714ec4bb76e3d474e14cac",
      "f6a48091bc4f4680acdd64f2a3fee5cc",
      "dcc947b1fa38423b8e579c10f4cad8c3",
      "f5f9b726114849978ff0f8ece12bcba9",
      "928ab0b08c8b4870ac945c5252dbf2ff"
     ]
    },
    "id": "CeS4fX3Jh_tM",
    "outputId": "4a21030d-3d27-4d63-8c06-ba1c10a85dda",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yorko/scibert_scivocab_uncased_long_4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def preprocess_function(examples):\n",
    "#    return tokenizer(examples[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encodings = dataset.map(preprocess_function, batched=True)\n",
    "train_encodings = tokenizer(train_dataset[\"text\"], truncation=True, padding=True, max_length=4096)\n",
    "val_encodings = tokenizer(val_dataset[\"text\"], truncation=True, padding=True, max_length=4096)\n",
    "test_encodings = tokenizer(test_dataset[\"text\"], truncation=True, padding=True, max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(974, 974, 974)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings[0]), len(train_encodings[1]), len(train_encodings[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505.3125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.sum([t == '[PAD]' for t in train_encodings[e].tokens]) for e in range(0,80)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = train_dataset[\"label\"]\n",
    "val_labels = val_dataset[\"label\"]\n",
    "test_labels = test_dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(100).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    ")).shuffle(100).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.10.attention.self.value_global.weight', 'bert.encoder.layer.7.attention.self.value_global.weight', 'bert.encoder.layer.9.attention.self.query_global.weight', 'bert.encoder.layer.4.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.key_global.weight', 'bert.encoder.layer.8.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.value_global.bias', 'bert.encoder.layer.1.attention.self.key_global.bias', 'bert.encoder.layer.9.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.query_global.weight', 'bert.encoder.layer.0.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.weight', 'bert.encoder.layer.5.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.key_global.bias', 'bert.encoder.layer.8.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.value_global.weight', 'bert.encoder.layer.3.attention.self.value_global.bias', 'bert.encoder.layer.5.attention.self.query_global.weight', 'bert.encoder.layer.9.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.weight', 'bert.encoder.layer.0.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.query_global.weight', 'bert.encoder.layer.10.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.query_global.bias', 'bert.encoder.layer.0.attention.self.query_global.weight', 'bert.encoder.layer.11.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.bias', 'bert.encoder.layer.3.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.key_global.bias', 'bert.encoder.layer.4.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.value_global.bias', 'bert.encoder.layer.9.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.key_global.bias', 'bert.encoder.layer.1.attention.self.query_global.bias', 'bert.encoder.layer.6.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.query_global.bias', 'bert.encoder.layer.1.attention.self.value_global.weight', 'bert.encoder.layer.11.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.weight', 'bert.encoder.layer.4.attention.self.key_global.bias', 'bert.encoder.layer.2.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.key_global.weight', 'bert.encoder.layer.6.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.value_global.weight', 'bert.encoder.layer.6.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.value_global.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-06    Trainable: True\n",
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  112670976 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,672,514\n",
      "Trainable params: 112,672,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 208s 29s/step - loss: 0.7084 - sparse_categorical_accuracy: 0.4706 - val_loss: 0.7214 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.6933 - sparse_categorical_accuracy: 0.5529 - val_loss: 0.7067 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.6802 - sparse_categorical_accuracy: 0.5412 - val_loss: 0.6942 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.6529 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6866 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.6307 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6792 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.6387 - sparse_categorical_accuracy: 0.6235 - val_loss: 0.6742 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.6330 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6700 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.6335 - sparse_categorical_accuracy: 0.6588 - val_loss: 0.6647 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.6314 - sparse_categorical_accuracy: 0.6471 - val_loss: 0.6622 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.6232 - sparse_categorical_accuracy: 0.6588 - val_loss: 0.6594 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.5910 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.6577 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.5989 - sparse_categorical_accuracy: 0.6235 - val_loss: 0.6537 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5964 - sparse_categorical_accuracy: 0.6471 - val_loss: 0.6504 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5849 - sparse_categorical_accuracy: 0.6824 - val_loss: 0.6479 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.6015 - sparse_categorical_accuracy: 0.6471 - val_loss: 0.6470 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5603 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.6438 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.5618 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.6399 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.5669 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.6363 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5534 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.6329 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5281 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.6276 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5154 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.6225 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.5146 - sparse_categorical_accuracy: 0.7882 - val_loss: 0.6151 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5103 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.6083 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5122 - sparse_categorical_accuracy: 0.8118 - val_loss: 0.6021 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.4958 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.5974 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.4777 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.5943 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.4968 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.5914 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.4784 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5879 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.4379 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5839 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.4429 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5835 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.4386 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5816 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.4189 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5794 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.4130 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.5744 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.3995 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.5732 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.4044 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.5725 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 167s 28s/step - loss: 0.3831 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.5676 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3664 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5638 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3728 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.5590 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.3681 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5597 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3604 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.5549 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3475 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.5563 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3305 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5533 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.3278 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.5479 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.3042 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5432 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3312 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5439 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3000 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5452 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3026 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5446 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.2933 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.5454 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.2933 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.5418 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.2747 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5401 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.2566 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.5375 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.2659 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.5307 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.2552 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.5280 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.2479 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5318 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.2387 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.5322 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.2275 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5343 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.2262 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.5323 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.2222 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.5223 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.2264 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.5207 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.2089 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5285 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5294 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.1957 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5231 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.1949 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5087 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.1955 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.5133 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.1944 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5299 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.1768 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5189 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.1771 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4944 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.1774 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4791 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.1599 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4884 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.1728 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5044 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.1591 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5093 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.1541 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.1564 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4940 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.1423 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4748 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1463 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4706 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1405 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4971 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1364 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5254 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1328 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4966 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1168 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4470 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1181 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4325 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1131 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4400 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1174 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1091 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4561 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1059 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4621 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1072 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4612 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1014 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4440 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.0960 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4340 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.0928 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4483 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.0904 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4589 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.0880 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4706 - val_sparse_categorical_accuracy: 0.8000\n",
      "1e-06 [90, [0.0903250053524971, 1.0], [0.43250420689582825, 0.800000011920929], [0.8045264482498169, 0.6499999761581421]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.10.attention.self.value_global.weight', 'bert.encoder.layer.7.attention.self.value_global.weight', 'bert.encoder.layer.9.attention.self.query_global.weight', 'bert.encoder.layer.4.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.key_global.weight', 'bert.encoder.layer.8.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.value_global.bias', 'bert.encoder.layer.1.attention.self.key_global.bias', 'bert.encoder.layer.9.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.query_global.weight', 'bert.encoder.layer.0.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.weight', 'bert.encoder.layer.5.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.key_global.bias', 'bert.encoder.layer.8.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.value_global.weight', 'bert.encoder.layer.3.attention.self.value_global.bias', 'bert.encoder.layer.5.attention.self.query_global.weight', 'bert.encoder.layer.9.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.weight', 'bert.encoder.layer.0.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.query_global.weight', 'bert.encoder.layer.10.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.query_global.bias', 'bert.encoder.layer.0.attention.self.query_global.weight', 'bert.encoder.layer.11.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.bias', 'bert.encoder.layer.3.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.key_global.bias', 'bert.encoder.layer.4.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.value_global.bias', 'bert.encoder.layer.9.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.key_global.bias', 'bert.encoder.layer.1.attention.self.query_global.bias', 'bert.encoder.layer.6.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.query_global.bias', 'bert.encoder.layer.1.attention.self.value_global.weight', 'bert.encoder.layer.11.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.weight', 'bert.encoder.layer.4.attention.self.key_global.bias', 'bert.encoder.layer.2.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.key_global.weight', 'bert.encoder.layer.6.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.value_global.weight', 'bert.encoder.layer.6.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.value_global.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1e-06    Trainable: True\n",
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  112670976 \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,672,514\n",
      "Trainable params: 112,672,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 196s 29s/step - loss: 0.6862 - sparse_categorical_accuracy: 0.5294 - val_loss: 0.6866 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.6537 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6761 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.6500 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6707 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6568 - sparse_categorical_accuracy: 0.6588 - val_loss: 0.6664 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.6473 - sparse_categorical_accuracy: 0.6588 - val_loss: 0.6626 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6373 - sparse_categorical_accuracy: 0.6941 - val_loss: 0.6586 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6298 - sparse_categorical_accuracy: 0.6824 - val_loss: 0.6553 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6364 - sparse_categorical_accuracy: 0.6706 - val_loss: 0.6518 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5948 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.6482 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6133 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.6453 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6057 - sparse_categorical_accuracy: 0.6706 - val_loss: 0.6425 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5918 - sparse_categorical_accuracy: 0.6941 - val_loss: 0.6400 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5998 - sparse_categorical_accuracy: 0.7294 - val_loss: 0.6367 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5934 - sparse_categorical_accuracy: 0.7294 - val_loss: 0.6336 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5750 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.6302 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5483 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.6269 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 178s 29s/step - loss: 0.5488 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.6238 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.5595 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.6200 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5519 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.6153 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5462 - sparse_categorical_accuracy: 0.7882 - val_loss: 0.6097 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5275 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.6046 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5366 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.6010 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5114 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.5964 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5253 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.5901 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.5067 - sparse_categorical_accuracy: 0.7882 - val_loss: 0.5858 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.4881 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.5828 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.4863 - sparse_categorical_accuracy: 0.8118 - val_loss: 0.5779 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.4736 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5715 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 167s 28s/step - loss: 0.4798 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5667 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.4541 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5623 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.4598 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5600 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.4399 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5546 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.4416 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5496 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.4362 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5486 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.4117 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5484 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.4074 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5470 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3967 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5436 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3970 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5362 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3855 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5310 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3869 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5272 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3571 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5257 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.3628 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5265 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.3479 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5222 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.3413 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5213 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.3473 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5146 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 177s 29s/step - loss: 0.3259 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.5111 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.3099 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.5109 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.2996 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5112 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 177s 29s/step - loss: 0.2935 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.5110 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.2937 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5045 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 177s 29s/step - loss: 0.2827 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5026 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.2682 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.5042 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.2518 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5081 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.2381 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.5063 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.2429 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5021 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.2357 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.4964 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.2187 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4871 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.2423 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4929 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.2188 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4933 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.2109 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4985 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.1961 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4982 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.1842 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5004 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.1915 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4977 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.1742 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4923 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.1729 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4832 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1816 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4824 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1662 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4813 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 167s 28s/step - loss: 0.1590 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4902 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1581 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4817 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1514 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4618 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1488 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4851 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1388 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4936 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.1214 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4741 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1213 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4531 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1213 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4405 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1185 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4339 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1133 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4411 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1049 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1113 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4782 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.1040 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4909 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.0944 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4817 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.0858 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4577 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.0933 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4766 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.0852 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4770 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.0791 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4621 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.0763 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4563 - val_sparse_categorical_accuracy: 0.8000\n",
      "1e-06 [86, [0.07857728004455566, 1.0], [0.43393853306770325, 0.800000011920929], [0.8213168978691101, 0.6399999856948853]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.10.attention.self.value_global.weight', 'bert.encoder.layer.7.attention.self.value_global.weight', 'bert.encoder.layer.9.attention.self.query_global.weight', 'bert.encoder.layer.4.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.key_global.weight', 'bert.encoder.layer.8.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.value_global.bias', 'bert.encoder.layer.1.attention.self.key_global.bias', 'bert.encoder.layer.9.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.query_global.weight', 'bert.encoder.layer.0.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.weight', 'bert.encoder.layer.5.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.key_global.bias', 'bert.encoder.layer.8.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.value_global.weight', 'bert.encoder.layer.3.attention.self.value_global.bias', 'bert.encoder.layer.5.attention.self.query_global.weight', 'bert.encoder.layer.9.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.weight', 'bert.encoder.layer.0.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.query_global.weight', 'bert.encoder.layer.10.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.query_global.bias', 'bert.encoder.layer.0.attention.self.query_global.weight', 'bert.encoder.layer.11.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.bias', 'bert.encoder.layer.3.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.key_global.bias', 'bert.encoder.layer.4.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.value_global.bias', 'bert.encoder.layer.9.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.key_global.bias', 'bert.encoder.layer.1.attention.self.query_global.bias', 'bert.encoder.layer.6.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.query_global.bias', 'bert.encoder.layer.1.attention.self.value_global.weight', 'bert.encoder.layer.11.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.weight', 'bert.encoder.layer.4.attention.self.key_global.bias', 'bert.encoder.layer.2.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.key_global.weight', 'bert.encoder.layer.6.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.value_global.weight', 'bert.encoder.layer.6.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.value_global.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2e-06    Trainable: True\n",
      "Model: \"tf_bert_for_sequence_classification_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  112670976 \n",
      "                                                                 \n",
      " dropout_113 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,672,514\n",
      "Trainable params: 112,672,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 203s 29s/step - loss: 0.6829 - sparse_categorical_accuracy: 0.6235 - val_loss: 0.6455 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.6370 - sparse_categorical_accuracy: 0.6471 - val_loss: 0.6424 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6331 - sparse_categorical_accuracy: 0.6706 - val_loss: 0.6355 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6172 - sparse_categorical_accuracy: 0.6588 - val_loss: 0.6271 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6103 - sparse_categorical_accuracy: 0.6706 - val_loss: 0.6199 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5992 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.6117 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5893 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.6047 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5678 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.5927 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5756 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.5806 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5460 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.5761 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.5281 - sparse_categorical_accuracy: 0.7882 - val_loss: 0.5671 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5118 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.5559 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.4959 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5447 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.4879 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5371 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.4870 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5305 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.4464 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5256 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.4539 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5233 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.4250 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5130 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.4070 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5066 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.3821 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5029 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.3799 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5013 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.3670 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.4971 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3467 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.4851 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.3266 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.4800 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3032 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.4813 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.2914 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.4770 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.2784 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.4701 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.2762 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.4706 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.2527 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4785 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.2461 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4727 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.2174 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4740 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.2121 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.4757 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.1959 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4802 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1851 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4819 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.1794 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4747 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.1614 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4614 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 167s 28s/step - loss: 0.1502 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4614 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1367 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4680 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.1295 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4733 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1220 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4769 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.1258 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4810 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.1021 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4892 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.0938 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4986 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.0914 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5019 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0844 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4948 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.0805 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4983 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.0706 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5020 - val_sparse_categorical_accuracy: 0.8667\n",
      "2e-06 [47, [0.10776659846305847, 0.9882352948188782], [0.4614223837852478, 0.8666666746139526], [0.8010843396186829, 0.6600000262260437]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.10.attention.self.value_global.weight', 'bert.encoder.layer.7.attention.self.value_global.weight', 'bert.encoder.layer.9.attention.self.query_global.weight', 'bert.encoder.layer.4.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.key_global.weight', 'bert.encoder.layer.8.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.value_global.bias', 'bert.encoder.layer.1.attention.self.key_global.bias', 'bert.encoder.layer.9.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.query_global.weight', 'bert.encoder.layer.0.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.weight', 'bert.encoder.layer.5.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.key_global.bias', 'bert.encoder.layer.8.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.value_global.weight', 'bert.encoder.layer.3.attention.self.value_global.bias', 'bert.encoder.layer.5.attention.self.query_global.weight', 'bert.encoder.layer.9.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.weight', 'bert.encoder.layer.0.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.query_global.weight', 'bert.encoder.layer.10.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.query_global.bias', 'bert.encoder.layer.0.attention.self.query_global.weight', 'bert.encoder.layer.11.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.bias', 'bert.encoder.layer.3.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.key_global.bias', 'bert.encoder.layer.4.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.value_global.bias', 'bert.encoder.layer.9.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.key_global.bias', 'bert.encoder.layer.1.attention.self.query_global.bias', 'bert.encoder.layer.6.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.query_global.bias', 'bert.encoder.layer.1.attention.self.value_global.weight', 'bert.encoder.layer.11.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.weight', 'bert.encoder.layer.4.attention.self.key_global.bias', 'bert.encoder.layer.2.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.key_global.weight', 'bert.encoder.layer.6.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.value_global.weight', 'bert.encoder.layer.6.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.value_global.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2e-06    Trainable: True\n",
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  112670976 \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,672,514\n",
      "Trainable params: 112,672,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 201s 29s/step - loss: 0.7632 - sparse_categorical_accuracy: 0.3765 - val_loss: 0.7288 - val_sparse_categorical_accuracy: 0.3333\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.7209 - sparse_categorical_accuracy: 0.4588 - val_loss: 0.7038 - val_sparse_categorical_accuracy: 0.5333\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6920 - sparse_categorical_accuracy: 0.5529 - val_loss: 0.6876 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6734 - sparse_categorical_accuracy: 0.5529 - val_loss: 0.6712 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.6717 - sparse_categorical_accuracy: 0.5765 - val_loss: 0.6593 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.6577 - sparse_categorical_accuracy: 0.6118 - val_loss: 0.6512 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6649 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6453 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6292 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6413 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.6291 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6363 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.6306 - sparse_categorical_accuracy: 0.6824 - val_loss: 0.6302 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.5885 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.6238 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5971 - sparse_categorical_accuracy: 0.6706 - val_loss: 0.6164 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.5718 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.6104 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.5643 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.6048 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.5279 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.5981 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.5267 - sparse_categorical_accuracy: 0.7647 - val_loss: 0.5879 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.5091 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5796 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.4813 - sparse_categorical_accuracy: 0.8353 - val_loss: 0.5697 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.4596 - sparse_categorical_accuracy: 0.8471 - val_loss: 0.5584 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.4314 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5483 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.4110 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5336 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3822 - sparse_categorical_accuracy: 0.8588 - val_loss: 0.5206 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.3574 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.5081 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.3253 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5173 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.2994 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.4637 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.2854 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.4925 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.2398 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.4270 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.2087 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4011 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.1992 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5393 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.1724 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4263 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1548 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.4168 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1387 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4626 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1250 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5152 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0979 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.3386 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.0955 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.3230 - val_sparse_categorical_accuracy: 0.8667\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.0789 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4731 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0716 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4956 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0526 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4032 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.0838 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.7224 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0435 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4329 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.0467 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4457 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0402 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6045 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.0401 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6188 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.0330 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5335 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.0298 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5226 - val_sparse_categorical_accuracy: 0.7333\n",
      "2e-06 [45, [0.04980556294322014, 1.0], [0.3230489194393158, 0.8666666746139526], [0.8808179497718811, 0.6600000262260437]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.10.attention.self.value_global.weight', 'bert.encoder.layer.7.attention.self.value_global.weight', 'bert.encoder.layer.9.attention.self.query_global.weight', 'bert.encoder.layer.4.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.key_global.weight', 'bert.encoder.layer.8.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.value_global.bias', 'bert.encoder.layer.1.attention.self.key_global.bias', 'bert.encoder.layer.9.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.query_global.weight', 'bert.encoder.layer.0.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.weight', 'bert.encoder.layer.5.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.key_global.bias', 'bert.encoder.layer.8.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.value_global.weight', 'bert.encoder.layer.3.attention.self.value_global.bias', 'bert.encoder.layer.5.attention.self.query_global.weight', 'bert.encoder.layer.9.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.weight', 'bert.encoder.layer.0.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.query_global.weight', 'bert.encoder.layer.10.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.query_global.bias', 'bert.encoder.layer.0.attention.self.query_global.weight', 'bert.encoder.layer.11.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.bias', 'bert.encoder.layer.3.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.key_global.bias', 'bert.encoder.layer.4.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.value_global.bias', 'bert.encoder.layer.9.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.key_global.bias', 'bert.encoder.layer.1.attention.self.query_global.bias', 'bert.encoder.layer.6.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.query_global.bias', 'bert.encoder.layer.1.attention.self.value_global.weight', 'bert.encoder.layer.11.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.weight', 'bert.encoder.layer.4.attention.self.key_global.bias', 'bert.encoder.layer.2.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.key_global.weight', 'bert.encoder.layer.6.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.value_global.weight', 'bert.encoder.layer.6.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.value_global.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-06    Trainable: True\n",
      "Model: \"tf_bert_for_sequence_classification_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  112670976 \n",
      "                                                                 \n",
      " dropout_189 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,672,514\n",
      "Trainable params: 112,672,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 203s 29s/step - loss: 0.6828 - sparse_categorical_accuracy: 0.5647 - val_loss: 0.6668 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6482 - sparse_categorical_accuracy: 0.6235 - val_loss: 0.6467 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.6182 - sparse_categorical_accuracy: 0.6353 - val_loss: 0.6334 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.5935 - sparse_categorical_accuracy: 0.6471 - val_loss: 0.6149 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.5525 - sparse_categorical_accuracy: 0.7059 - val_loss: 0.6010 - val_sparse_categorical_accuracy: 0.6000\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.5314 - sparse_categorical_accuracy: 0.7765 - val_loss: 0.5800 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.4908 - sparse_categorical_accuracy: 0.8000 - val_loss: 0.5535 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.4277 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.5197 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.4164 - sparse_categorical_accuracy: 0.8235 - val_loss: 0.5233 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.3132 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.4948 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 173s 28s/step - loss: 0.2705 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.5414 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.2110 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.4702 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 171s 28s/step - loss: 0.1587 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4736 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.1103 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.4964 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.0709 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5428 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.0497 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5978 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.0389 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6280 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.0308 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6824 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.0259 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7038 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.0209 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6746 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 170s 28s/step - loss: 0.0177 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7200 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.0189 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7844 - val_sparse_categorical_accuracy: 0.7333\n",
      "5e-06 [22, [0.13026946783065796, 0.9764705896377563], [0.47020742297172546, 0.800000011920929], [0.7432669997215271, 0.6399999856948853]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.encoder.layer.10.attention.self.value_global.weight', 'bert.encoder.layer.7.attention.self.value_global.weight', 'bert.encoder.layer.9.attention.self.query_global.weight', 'bert.encoder.layer.4.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.query_global.weight', 'bert.encoder.layer.2.attention.self.key_global.weight', 'bert.encoder.layer.8.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.value_global.bias', 'bert.encoder.layer.1.attention.self.key_global.bias', 'bert.encoder.layer.9.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.query_global.weight', 'bert.encoder.layer.0.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.key_global.weight', 'bert.encoder.layer.5.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.key_global.bias', 'bert.encoder.layer.8.attention.self.key_global.bias', 'bert.encoder.layer.0.attention.self.value_global.weight', 'bert.encoder.layer.3.attention.self.value_global.bias', 'bert.encoder.layer.5.attention.self.query_global.weight', 'bert.encoder.layer.9.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.bias', 'bert.encoder.layer.10.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.query_global.bias', 'bert.encoder.layer.4.attention.self.value_global.weight', 'bert.encoder.layer.0.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.bias', 'bert.encoder.layer.10.attention.self.query_global.weight', 'bert.encoder.layer.10.attention.self.key_global.weight', 'bert.encoder.layer.9.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.bias', 'bert.encoder.layer.8.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.query_global.bias', 'bert.encoder.layer.0.attention.self.query_global.weight', 'bert.encoder.layer.11.attention.self.key_global.weight', 'bert.encoder.layer.3.attention.self.query_global.weight', 'bert.encoder.layer.1.attention.self.key_global.weight', 'bert.encoder.layer.11.attention.self.query_global.bias', 'bert.encoder.layer.3.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.key_global.bias', 'bert.encoder.layer.4.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.query_global.bias', 'bert.encoder.layer.7.attention.self.key_global.bias', 'bert.encoder.layer.11.attention.self.value_global.bias', 'bert.encoder.layer.9.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.query_global.weight', 'bert.encoder.layer.3.attention.self.key_global.bias', 'bert.encoder.layer.1.attention.self.query_global.bias', 'bert.encoder.layer.6.attention.self.key_global.bias', 'bert.encoder.layer.5.attention.self.query_global.bias', 'bert.encoder.layer.1.attention.self.value_global.weight', 'bert.encoder.layer.11.attention.self.value_global.weight', 'bert.encoder.layer.8.attention.self.value_global.weight', 'bert.encoder.layer.5.attention.self.value_global.weight', 'bert.encoder.layer.4.attention.self.key_global.bias', 'bert.encoder.layer.2.attention.self.value_global.bias', 'bert.encoder.layer.7.attention.self.key_global.weight', 'bert.encoder.layer.6.attention.self.key_global.weight', 'bert.encoder.layer.2.attention.self.value_global.weight', 'bert.encoder.layer.6.attention.self.value_global.bias', 'bert.encoder.layer.6.attention.self.value_global.weight']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5e-06    Trainable: True\n",
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  112670976 \n",
      "                                                                 \n",
      " dropout_227 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 112,672,514\n",
      "Trainable params: 112,672,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "6/6 [==============================] - 197s 28s/step - loss: 0.6707 - sparse_categorical_accuracy: 0.6000 - val_loss: 0.6450 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.6362 - sparse_categorical_accuracy: 0.6706 - val_loss: 0.6216 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.6209 - sparse_categorical_accuracy: 0.6588 - val_loss: 0.6042 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.5963 - sparse_categorical_accuracy: 0.7176 - val_loss: 0.5936 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.5619 - sparse_categorical_accuracy: 0.7294 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 172s 28s/step - loss: 0.5194 - sparse_categorical_accuracy: 0.8118 - val_loss: 0.5491 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.4955 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5316 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.4533 - sparse_categorical_accuracy: 0.8824 - val_loss: 0.5217 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.4309 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.5116 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.4010 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5067 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.3477 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.3106 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.4811 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.2720 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.4851 - val_sparse_categorical_accuracy: 0.7333\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.2441 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.4829 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 173s 29s/step - loss: 0.2201 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.4960 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.2069 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.4909 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.1763 - sparse_categorical_accuracy: 0.9765 - val_loss: 0.4913 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.1580 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5282 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.1275 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.5569 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0985 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5313 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 174s 29s/step - loss: 0.0897 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5400 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.0683 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5737 - val_sparse_categorical_accuracy: 0.8000\n",
      "5e-06 [22, [0.25061240792274475, 0.9411764740943909], [0.48111286759376526, 0.800000011920929], [0.7009721398353577, 0.6299999952316284]]\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load BERT model\n",
    "Trainable = True\n",
    "for lr in [1e-4, 2e-4, 5e-4, 1e-5, 2e-5, 5e-5, 1e-6, 2e-6, 5e-6]:\n",
    "    for Ismetles in range (0,5):\n",
    "        TestEredmeny = func_betolt(lr, Trainable, train_dataset, val_dataset, test_dataset, tokenizer)\n",
    "        print(lr,  TestEredmeny)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TextClassificationDS2A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "09d3283c59714ec4bb76e3d474e14cac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_928ab0b08c8b4870ac945c5252dbf2ff",
      "placeholder": "",
      "style": "IPY_MODEL_f5f9b726114849978ff0f8ece12bcba9",
      "value": " 228k/228k [00:02&lt;00:00, 76.8kB/s]"
     }
    },
    "0e5baf42d613466c9789d6ed74223515": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "24a91bb3dd86425ebe0dd489d07a99e6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "382bcdaba94748dfba4de9ac6df3d0fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4136d11d23304f44a6fd77bd5078c678": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a38b3162339e4d49a7c705818d3d9014",
       "IPY_MODEL_b7a8f10c56f84fce8fd9e0325b3c444e"
      ],
      "layout": "IPY_MODEL_858f4eafdb214acd80236294e009f285"
     }
    },
    "49684e0c97d34a558c268306fbbdf3f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b033809b6f434f12b338b3ac2119da37",
       "IPY_MODEL_09d3283c59714ec4bb76e3d474e14cac"
      ],
      "layout": "IPY_MODEL_ac8ce475408a49669d263c85042f9530"
     }
    },
    "59e835dce3754a3da47bd3d3106843a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c10fdb6c1963442d8dc23bfa989e8183",
      "max": 442221694,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b11ffa580c40481890990666b9813e33",
      "value": 442221694
     }
    },
    "858f4eafdb214acd80236294e009f285": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "928ab0b08c8b4870ac945c5252dbf2ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dd6e58d01ff4bcba634f49e52de97b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_59e835dce3754a3da47bd3d3106843a8",
       "IPY_MODEL_b1c95a342bd149aa9940cb4b6290f6cc"
      ],
      "layout": "IPY_MODEL_9e1afcdfeaf4406e860f74b6d4fa5634"
     }
    },
    "9e1afcdfeaf4406e860f74b6d4fa5634": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a38b3162339e4d49a7c705818d3d9014": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbfb95452bce4570805ef71e5fce0121",
      "max": 385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e5baf42d613466c9789d6ed74223515",
      "value": 385
     }
    },
    "a3cddd07fc284418982f9bd6cba0e89f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac8ce475408a49669d263c85042f9530": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b033809b6f434f12b338b3ac2119da37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcc947b1fa38423b8e579c10f4cad8c3",
      "max": 227845,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6a48091bc4f4680acdd64f2a3fee5cc",
      "value": 227845
     }
    },
    "b11ffa580c40481890990666b9813e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b1c95a342bd149aa9940cb4b6290f6cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24a91bb3dd86425ebe0dd489d07a99e6",
      "placeholder": "",
      "style": "IPY_MODEL_382bcdaba94748dfba4de9ac6df3d0fc",
      "value": " 442M/442M [00:12&lt;00:00, 34.2MB/s]"
     }
    },
    "b7a8f10c56f84fce8fd9e0325b3c444e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1fd9a54bdb847a3a4ac6a9e49f8bea6",
      "placeholder": "",
      "style": "IPY_MODEL_a3cddd07fc284418982f9bd6cba0e89f",
      "value": " 385/385 [00:01&lt;00:00, 323B/s]"
     }
    },
    "c10fdb6c1963442d8dc23bfa989e8183": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbfb95452bce4570805ef71e5fce0121": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1fd9a54bdb847a3a4ac6a9e49f8bea6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcc947b1fa38423b8e579c10f4cad8c3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5f9b726114849978ff0f8ece12bcba9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6a48091bc4f4680acdd64f2a3fee5cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
